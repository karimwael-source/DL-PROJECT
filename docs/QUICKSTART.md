# ğŸš€ Quick Start Guide

## âœ… Step 1: Test Model (No Dataset Needed)

Test if the model architecture works:

```bash
python test_model.py
```

This will:
- Create the model
- Test forward pass with dummy data
- Verify keyframe selection
- Test ResNet unfreezing

**Expected output**: "ALL TESTS PASSED - MODEL WORKS PERFECTLY!"

---

## ğŸ“¥ Step 2: Get TVSum Dataset

You need to download TVSum dataset manually:

### Option A: Official Source
```bash
# Clone the TVSum repo
git clone https://github.com/yalesong/tvsum

# Or download directly from:
# https://github.com/yalesong/tvsum/tree/master/data
```

### Option B: Alternative Sources
- Search for "TVSum dataset download" 
- Look for `ydata-tvsum50-v1_1.zip`
- Extract to get `ydata-tvsum50.mat` or `tvsum.h5`

### Expected Structure:
```
E:/
â””â”€â”€ DL_project_finalized/
    â”œâ”€â”€ model.py
    â”œâ”€â”€ dataset.py
    â”œâ”€â”€ train.py
    â”œâ”€â”€ ...
    â””â”€â”€ data/              # Create this folder
        â”œâ”€â”€ tvsum/
        â”‚   â”œâ”€â”€ videos/    # Put video files here
        â”‚   â”‚   â”œâ”€â”€ video_1.mp4
        â”‚   â”‚   â”œâ”€â”€ video_2.mp4
        â”‚   â”‚   â””â”€â”€ ...
        â”‚   â””â”€â”€ tvsum.h5   # Annotation file
```

---

## ğŸ§ª Step 3: Test Dataset Loading

Once you have the dataset:

```bash
python test_dataset.py
```

This will verify:
- Videos can be loaded
- Frames are sampled correctly (2 FPS = 60 frames)
- Annotations are loaded properly

---

## ğŸ‹ï¸ Step 4: Start Training

```bash
python train.py \
    --video_dir E:/DL_project_finalized/data/tvsum/videos \
    --h5_path E:/DL_project_finalized/data/tvsum/tvsum.h5 \
    --batch_size 4 \
    --epochs_stage1 10 \
    --epochs_stage2 20
```

**For Google Colab** (if dataset is on Google Drive):
```python
from google.colab import drive
drive.mount('/content/drive')

!python train.py \
    --video_dir /content/drive/MyDrive/tvsum/videos \
    --h5_path /content/drive/MyDrive/tvsum/tvsum.h5 \
    --batch_size 2
```

---

## ğŸ“Š Step 5: Monitor Training

Open TensorBoard:
```bash
tensorboard --logdir logs
```

Then open: http://localhost:6006

---

## ğŸ¨ Step 6: Visualize Results

After training completes:

```bash
python visualize.py \
    --video_dir E:/DL_project_finalized/data/tvsum/videos \
    --h5_path E:/DL_project_finalized/data/tvsum/tvsum.h5 \
    --checkpoint checkpoints/best_model.pth \
    --num_videos 5
```

Check `visualizations/` folder for:
- Importance curve plots
- Keyframe grids
- Individual keyframes

---

## âš¡ Quick Check (Before Full Training)

If you want to test everything quickly:

1. **Install packages**:
   ```bash
   pip install -r requirements.txt
   ```

2. **Test model** (no dataset needed):
   ```bash
   python test_model.py
   ```

3. **Get dataset** (manual download)

4. **Test dataset loading**:
   ```bash
   python test_dataset.py
   ```

5. **Start training!**

---

## â“ FAQ

**Q: Do I need an API key?**  
A: No! The data loads from local files, no API needed.

**Q: Where to download TVSum?**  
A: Search "TVSum dataset" or check GitHub: https://github.com/yalesong/tvsum

**Q: Can I use a different dataset?**  
A: Yes, but you need to modify `dataset.py` to match your format.

**Q: How long does training take?**  
A: ~2-3 hours on GPU (Colab), 10+ hours on CPU (not recommended).

**Q: Out of memory error?**  
A: Reduce `--batch_size 2` or `--batch_size 1`

**Q: Can I test with 1 video only?**  
A: Yes! Modify the dataset split in `dataset.py` or create a custom test script.

---

## ğŸ¯ Summary

1. âœ… Test model â†’ `python test_model.py`
2. ğŸ“¥ Download TVSum dataset manually
3. ğŸ§ª Test dataset â†’ `python test_dataset.py`
4. ğŸ‹ï¸ Train â†’ `python train.py --video_dir ... --h5_path ...`
5. ğŸ“Š Monitor â†’ `tensorboard --logdir logs`
6. ğŸ¨ Visualize â†’ `python visualize.py --checkpoint ...`

**No API needed - everything runs locally!**
